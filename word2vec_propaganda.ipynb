{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_propaganda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu6kTMmswbLS"
      },
      "source": [
        "**word2vec **\\\n",
        "**x = 2 (Text), y = 1 (Propaganda_devices)**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow_text"
      ],
      "metadata": {
        "id": "g0qbUiK7ZAz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read dataset"
      ],
      "metadata": {
        "id": "FX11nZgWkmxO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QvJ0NrRkXQf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "99199d80-0a04-46cb-f0e4-2fb12ed1b0af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "#from tensorflow import keras\n",
        "import numpy as np\n",
        "import gensim\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/propaganda/sonal_shyam_icolsi_2021/propaganda_data.csv\")\n",
        "df.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  tag\n",
              "0  अमेरिका और चीन के लिए जंग का मैदान बन चुके ताइ...    0\n",
              "1  अमेरिका के स्वास्थ्य और मानव सेवा मंत्री एलेक्...    0\n",
              "2                    ताइवान ने इसका करारा जवाब दिया।    0\n",
              "3  ताइवान स्ट्रेट मिड-लाइन को जैसे ही चीनी लड़ाकू ...    0\n",
              "4                           इसके बाद से भाग्ग निकले।    0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f74e96f5-c977-4bf5-ac65-ea78d7ed7bff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>अमेरिका और चीन के लिए जंग का मैदान बन चुके ताइ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>अमेरिका के स्वास्थ्य और मानव सेवा मंत्री एलेक्...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ताइवान ने इसका करारा जवाब दिया।</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ताइवान स्ट्रेट मिड-लाइन को जैसे ही चीनी लड़ाकू ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>इसके बाद से भाग्ग निकले।</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f74e96f5-c977-4bf5-ac65-ea78d7ed7bff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f74e96f5-c977-4bf5-ac65-ea78d7ed7bff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f74e96f5-c977-4bf5-ac65-ea78d7ed7bff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRNiCd3gM30x"
      },
      "source": [
        "X_train = df.loc[:12510, 'text'].values\n",
        "y_train = df.loc[:12510, 'tag'].values\n",
        "X_test = df.loc[:12511:, 'text'].values\n",
        "y_test = df.loc[:12511:, 'tag'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# tokenizer_obj = Tokenizer()\n",
        "# total_texts = X_train + X_test\n",
        "# tokenizer_obj.fit_on_texts(total_texts)\n",
        "\n",
        "# #pad sequences\n",
        "# max_length = max([len(s.split()) for s in total_texts])\n",
        "\n",
        "# # define vocabulary size\n",
        "# vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "# X_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\n",
        "# X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
        "\n",
        "# X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "# X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "vFFYwZV14E_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "a4aed152-2286-42ad-b1c4-71d8cb25de4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-38b8ba641a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tokenizer_obj = Tokenizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# total_texts = X_train + X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.keras.preprocessing'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
        "\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "# from keras.layers.embeddings import Embedding\n",
        "\n",
        "# EMBEDDING_DIM = 100\n",
        "\n",
        "# print('Build model...')\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
        "# model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lx6-QzOoEDd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.summary()"
      ],
      "metadata": {
        "id": "j10TWVzWUyRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.fit(X_train_pad, y_train, batch_size=128, epochs=2, validation_data=(X_test_pad, y_test), verbose=2)"
      ],
      "metadata": {
        "id": "blC5BJGIlSry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test model \n",
        "\n",
        "# test_sam_1 = \"उनकी पार्टी एक जिम्मेदार विपक्ष के तौर पर काम करेगी\"\n",
        "# test_sam_2 = \"शिवसेना सीएम पद के लिए अड़ी रही\"\n",
        "# test_sam_3 = \"कांग्रेस को एक जिम्मेदार विपक्ष होने का जनादेश मिला है\"\n",
        "# test_sam_4 = \"वहीं निफ्टी 5.70 अंक यानी 0.05 फीसदी की गिरावट के बाद 11,911.50 के स्तर पर था।\"\n",
        "# test_sam_5 = \"सेंसेक्स 63.62 अंक यानी 0.16 फीसदी की गिरावट के बाद 40,311.85 के स्तर पर था।\"\n",
        "# test_samples = [test_sam_1, test_sam_2, test_sam_3, test_sam_4, test_sam_5]\n",
        "# test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
        "# test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n",
        "\n",
        "# #predict\n",
        "# model.predict(x=test_samples_tokens_pad)"
      ],
      "metadata": {
        "id": "vdQxFQQqxqqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GFh-y1xFxiLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# METRICS = [\n",
        "#            tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "#            tf.keras.metrics.Precision(name='precision'),\n",
        "#            tf.keras.metrics.Recall(name='recall')\n",
        "\n",
        "# ]\n",
        "\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=METRICS)"
      ],
      "metadata": {
        "id": "mptLBGxSYfeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrJ5L73yDPoR"
      },
      "source": [
        "#print('Train...')\n",
        "\n",
        "# model.fit(X_train_pad, y_train, batch_size=128, epochs=2, validation_data=(X_test_pad, y_test), verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.evaluate(X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "1doaReNIibx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_predicted = model.predict(X_test_pad)\n",
        "# y_predicted = y_predicted.flatten()"
      ],
      "metadata": {
        "id": "De_ENHE6iw2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "hTTPtr3HjW0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_predicted"
      ],
      "metadata": {
        "id": "wFkabYkukZES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_test"
      ],
      "metadata": {
        "id": "d5FUkZzxkcd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(classification_report(y_test, y_predicted))"
      ],
      "metadata": {
        "id": "oE1IOyr6i2_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**USE GENSIM and train word2vec**"
      ],
      "metadata": {
        "id": "YE2qNhQD1j-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text_lines = list()\n",
        "lines = df['text'].values.tolist()\n",
        "#print(text_lines)\n",
        "\n",
        "for line in lines:\n",
        "  tokens = word_tokenize(line)\n",
        "  #print(len(tokens))\n",
        "  #print(tokens)\n",
        "  # convert to lower case\n",
        "  #tokens = [w.lower() for w in tokens]\n",
        "  #print(tokens)\n",
        "  # remove punctuation from each word \n",
        "  # table = str.maketrans('', '', string.punctuation)\n",
        "  # stripped = [w.translate(table) for w in tokens]\n",
        "  # #print(len(stripped))\n",
        "  # remove remaining tokens\n",
        "  #words = [word for word in stripped if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  #stop_words = set(stopwords.words('english'))\n",
        "  #words = [w for w in words if not w in stop_words]\n",
        "  text_lines.append(tokens)\n",
        "\n",
        "#print(text_lines)\n"
      ],
      "metadata": {
        "id": "vzT95j7F1hcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57f9940-5d34-403b-e4fb-7815b9e5a7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_lines)"
      ],
      "metadata": {
        "id": "uBfWXjZMVj2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf7dd99-5f05-4d90-c2d3-5d337cdf3299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12511"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "EMBEDDING_DIM = 100\n",
        "#train word2vec model\n",
        "model = gensim.models.Word2Vec(sentences=text_lines, size=EMBEDDING_DIM, window=5, workers =4, min_count= 0)\n",
        "# vocab size\n",
        "words = list(model.wv.vocab)\n",
        "print('Vocabulary size: %d' %len(words))"
      ],
      "metadata": {
        "id": "Elz7tdgwZK-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2add5013-9e80-42ef-bfe9-964a232c2370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 21367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "filename = 'prop_embedding_wordwvec.txt'\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "metadata": {
        "id": "hleV5RtUbwkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('', 'prop_embedding_wordwvec.txt'), encoding = 'utf-8')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:])\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()          "
      ],
      "metadata": {
        "id": "TwLPLmquccUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer  the text samples into a 2D integer tensor\n",
        "\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(text_lines)\n",
        "sequences = tokenizer_obj.texts_to_sequences(text_lines)\n",
        "\n",
        "#pad sequneces \n",
        "word_index = tokenizer_obj.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "text_pad = pad_sequences(sequences, maxlen=max_length)\n",
        "tag = df['tag'].values\n",
        "print('shape of text tensor:', text_pad.shape)\n",
        "print('Shape of tag tensor:', tag.shape)"
      ],
      "metadata": {
        "id": "1l691J8JdWSh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "d53f7106-c940-4b0b-87a3-203593e8aa89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-cfe7a7a117fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# vectorizer  the text samples into a 2D integer tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.keras.preprocessing'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        " \n",
        "for word, i in word_index.items():\n",
        "  if i > num_words:\n",
        "    continue\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "dPmXO0JDhHF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_words)"
      ],
      "metadata": {
        "id": "P0sZvL6kjp1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "# EMBEDDING_DIM = 100\n",
        "\n",
        "# print('Build model...')\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(num_words, \n",
        "                            EMBEDDING_DIM, \n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Drxyyl8kj9xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "kF8r3pDAnvjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into a training set and a validation set \n",
        "VALIDATION_SPLIT = 0.3\n",
        "\n",
        "indices = np.arange(text_pad.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "text_pad = text_pad[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT * text_pad.shape[0])\n",
        "\n",
        "X_train_pad = text_pad[:-num_validation_samples]\n",
        "y_train = tag[:-num_validation_samples]\n",
        "X_test_pad = text_pad[-num_validation_samples:]\n",
        "y_test = tag[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "2yNEgfjpz11y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
        "print('Shape of y_train tensor:', y_train.shape)\n",
        "\n",
        "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
        "print('Shape of y_test tensor:', y_test.shape)"
      ],
      "metadata": {
        "id": "GIM_eZE-2DLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('Train...')\n",
        "\n",
        "# model.fit(X_train_pad, y_train, batch_size=128, epochs=5, validation_data=(X_test_pad, y_test), verbose=2)"
      ],
      "metadata": {
        "id": "sInh_LhF3OEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "           tf.keras.metrics.Precision(name='precision'),\n",
        "           tf.keras.metrics.Recall(name='recall')\n",
        "\n",
        "]\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=METRICS)"
      ],
      "metadata": {
        "id": "Z8a_JacWevcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train...')\n",
        "\n",
        "model.fit(X_train_pad, y_train, batch_size=128, epochs=5, validation_data=(X_test_pad, y_test), verbose=2)"
      ],
      "metadata": {
        "id": "Yo-y5u2wf9Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_pad, y_test)"
      ],
      "metadata": {
        "id": "NnukSy3afbwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict(X_test_pad)\n",
        "y_predicted = y_predicted.flatten()"
      ],
      "metadata": {
        "id": "SYGEZNG1fiQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
        "y_predicted"
      ],
      "metadata": {
        "id": "R1bOz1Au_XEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_predicted))"
      ],
      "metadata": {
        "id": "ns1j1Hs6gTcV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}